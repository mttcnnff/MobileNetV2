{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mobilenet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mttcnnff/MobileNetV2/blob/master/src/mobilenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia1OiW2DsGwI",
        "colab_type": "code",
        "outputId": "987e67a4-3e1d-4e8f-f1ae-2084e5494367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/mttcnnff/MobileNetV2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MobileNetV2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQW_w0T2sNww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup path\n",
        "import sys\n",
        "sys.path.append('/content/MobileNetV2/src')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ_5yeXfr7lb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "24edd214-ce7d-45a2-ee9f-946e4a60ecc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from mobilenet import mobilenet_v2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "LABELS = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
        "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
        "    'worm'\n",
        "]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0702 11:02:02.758623 140324357187456 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0702 11:02:02.767011 140324357187456 deprecation_wrapper.py:119] From /content/MobileNetV2/src/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jazD-1cz1Pm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "b0ca34b2-56b8-4205-c4fb-f61468c25992"
      },
      "source": [
        "# Load a given dataset by name, along with the DatasetInfo\n",
        "data, info = tfds.load(\"cifar100\", with_info=True)\n",
        "train_data, test_data = data['train'], data['test']\n",
        "assert isinstance(train_data, tf.data.Dataset)\n",
        "assert info.features['label'].num_classes == 100\n",
        "assert info.splits['train'].num_examples == 50000\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Initialize placeholders \n",
        "x = tf.placeholder(dtype = tf.float32, shape = [None, 32, 32, 3])\n",
        "y = tf.placeholder(dtype = tf.int32, shape = [None])\n",
        "\n",
        "# Fully connected layer \n",
        "with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n",
        "  logits, endpoints = mobilenet_v2.mobilenet(x, num_classes=100)\n",
        "\n",
        "# Define a loss function\n",
        "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, \n",
        "                                                                    logits = logits))\n",
        "# Define an optimizer \n",
        "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
        "\n",
        "# Convert logits to label indexes\n",
        "correct_pred = tf.argmax(logits, 1)\n",
        "\n",
        "# Define an accuracy metric\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initializing the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(10):\n",
        "        # Loop over all batches\n",
        "        n_batches = 50\n",
        "        \n",
        "#         for batch_i in range(1, n_batches + 1):\n",
        "#             for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
        "#                 train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
        "                \n",
        "#             print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
        "#             print_stats(sess, batch_features, batch_labels, cost, accuracy)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-4a9b0e9262a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Initializing the variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Training cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OUNAEprr7ln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "8f8363a9-d4ea-48fa-e7c7-b413bf7311f7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# batched = train_data.batch(1000)\n",
        "\n",
        "# iter = batched.make_one_shot_iterator()\n",
        "# x = iter.get_next()\n",
        "\n",
        "# type(x)\n",
        "\n",
        "\n",
        " # Load a given dataset by name, along with the DatasetInfo\n",
        "data, info = tfds.load(\"cifar100\", with_info=True)\n",
        "print(info)\n",
        "train_data, test_data = data['train'], data['test']\n",
        "assert isinstance(train_data, tf.data.Dataset)\n",
        "assert info.features['label'].num_classes == 100\n",
        "assert info.splits['train'].num_examples == 50000\n",
        "\n",
        "dataset = train_data\n",
        "first_100 = dataset.range(100)\n",
        "batched_dataset = dataset.batch(4)\n",
        "iterator = batched_dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "  \n",
        "with tf.Session() as sess:\n",
        "  print(sess.run(next_element)['label'])  # ==> ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])\n",
        "  "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='cifar100',\n",
            "    version=1.3.1,\n",
            "    description='This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).',\n",
            "    urls=['https://www.cs.toronto.edu/~kriz/cifar.html'],\n",
            "    features=FeaturesDict({\n",
            "        'coarse_label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),\n",
            "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=100)\n",
            "    },\n",
            "    total_num_examples=60000,\n",
            "    splits={\n",
            "        'test': <tfds.core.SplitInfo num_examples=10000>,\n",
            "        'train': <tfds.core.SplitInfo num_examples=50000>\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation='\"\"\"\n",
            "        @TECHREPORT{Krizhevsky09learningmultiple,\n",
            "            author = {Alex Krizhevsky},\n",
            "            title = {Learning multiple layers of features from tiny images},\n",
            "            institution = {},\n",
            "            year = {2009}\n",
            "        }\n",
            "        \n",
            "    \"\"\"',\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n",
            "[97 41 77 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lqb_6mEKZkw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "22de2f96-6aff-4dd8-c6eb-e9088bf61134"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Load a given dataset by name, along with the DatasetInfo\n",
        "data, info = tfds.load(\"cifar100\", with_info=True)\n",
        "train_data, test_data = data['train'], data['test']\n",
        "assert isinstance(train_data, tf.data.Dataset)\n",
        "assert info.features['label'].num_classes == 100\n",
        "assert info.splits['train'].num_examples == 50000\n",
        "\n",
        "NUM_EPOCHS = 1\n",
        "NUM_EXAMPLES = 100\n",
        "BATCH_SIZE = 23\n",
        "NUM_BATCHES = NUM_EXAMPLES//BATCH_SIZE + 1\n",
        "ITERATIONS = NUM_EPOCHS * NUM_BATCHES\n",
        "dataset = train_data.take(NUM_EXAMPLES).repeat(NUM_EPOCHS)\n",
        "batched_dataset = dataset.batch(BATCH_SIZE)\n",
        "iterator = batched_dataset.make_one_shot_iterator()\n",
        "\n",
        "tf.set_random_seed(1234)\n",
        "with tf.Session() as sess:\n",
        "\n",
        "  # Initializing the variables\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  # Training cycle\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('EPOCH', epoch)\n",
        "    for i in range(NUM_BATCHES):\n",
        "      batch = sess.run(iterator.get_next())\n",
        "      x_train_tensor = tf.convert_to_tensor(batch['image'], dtype=tf.float32)\n",
        "      y_train_tensor = tf.convert_to_tensor(batch['label'], dtype=tf.int32)\n",
        "      \n",
        "      # Fully connected layer \n",
        "      with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n",
        "        logits, endpoints = mobilenet_v2.mobilenet(x_train_tensor, num_classes=100, reuse=tf.AUTO_REUSE)\n",
        "        print(endpoints.keys())\n",
        "#         # Define a loss function\n",
        "#         loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_train_tensor, \n",
        "#                                                                             logits = logits))\n",
        "#         # Define an optimizer \n",
        "#         train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
        "\n",
        "#         # Convert logits to label indexes\n",
        "#         correct_pred = tf.argmax(logits, 1)\n",
        "\n",
        "#         # Define an accuracy metric\n",
        "#         accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#         _, accuracy_val = sess.run([train_op, accuracy])\n",
        "#         tf.print(sess.run(loss))\n",
        "#         print(\"Loss: \", loss.eval())\n",
        "    print('DONE WITH EPOCH')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "    "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0\n",
            "dict_keys(['layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_2/depthwise_output', 'layer_2/output', 'layer_3/expansion_output', 'layer_3/depthwise_output', 'layer_3/output', 'layer_4/expansion_output', 'layer_4/depthwise_output', 'layer_4/output', 'layer_5/expansion_output', 'layer_5/depthwise_output', 'layer_5/output', 'layer_6/expansion_output', 'layer_6/depthwise_output', 'layer_6/output', 'layer_7/expansion_output', 'layer_7/depthwise_output', 'layer_7/output', 'layer_8/expansion_output', 'layer_8/depthwise_output', 'layer_8/output', 'layer_9/expansion_output', 'layer_9/depthwise_output', 'layer_9/output', 'layer_10/expansion_output', 'layer_10/depthwise_output', 'layer_10/output', 'layer_11/expansion_output', 'layer_11/depthwise_output', 'layer_11/output', 'layer_12/expansion_output', 'layer_12/depthwise_output', 'layer_12/output', 'layer_13/expansion_output', 'layer_13/depthwise_output', 'layer_13/output', 'layer_14/expansion_output', 'layer_14/depthwise_output', 'layer_14/output', 'layer_15/expansion_output', 'layer_15/depthwise_output', 'layer_15/output', 'layer_16/expansion_output', 'layer_16/depthwise_output', 'layer_16/output', 'layer_17/expansion_output', 'layer_17/depthwise_output', 'layer_17/output', 'layer_18/expansion_output', 'layer_18/depthwise_output', 'layer_18/output', 'global_pool', 'Logits', 'Predictions'])\n",
            "dict_keys(['layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_2/depthwise_output', 'layer_2/output', 'layer_3/expansion_output', 'layer_3/depthwise_output', 'layer_3/output', 'layer_4/expansion_output', 'layer_4/depthwise_output', 'layer_4/output', 'layer_5/expansion_output', 'layer_5/depthwise_output', 'layer_5/output', 'layer_6/expansion_output', 'layer_6/depthwise_output', 'layer_6/output', 'layer_7/expansion_output', 'layer_7/depthwise_output', 'layer_7/output', 'layer_8/expansion_output', 'layer_8/depthwise_output', 'layer_8/output', 'layer_9/expansion_output', 'layer_9/depthwise_output', 'layer_9/output', 'layer_10/expansion_output', 'layer_10/depthwise_output', 'layer_10/output', 'layer_11/expansion_output', 'layer_11/depthwise_output', 'layer_11/output', 'layer_12/expansion_output', 'layer_12/depthwise_output', 'layer_12/output', 'layer_13/expansion_output', 'layer_13/depthwise_output', 'layer_13/output', 'layer_14/expansion_output', 'layer_14/depthwise_output', 'layer_14/output', 'layer_15/expansion_output', 'layer_15/depthwise_output', 'layer_15/output', 'layer_16/expansion_output', 'layer_16/depthwise_output', 'layer_16/output', 'layer_17/expansion_output', 'layer_17/depthwise_output', 'layer_17/output', 'layer_18/expansion_output', 'layer_18/depthwise_output', 'layer_18/output', 'global_pool', 'Logits', 'Predictions'])\n",
            "dict_keys(['layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_2/depthwise_output', 'layer_2/output', 'layer_3/expansion_output', 'layer_3/depthwise_output', 'layer_3/output', 'layer_4/expansion_output', 'layer_4/depthwise_output', 'layer_4/output', 'layer_5/expansion_output', 'layer_5/depthwise_output', 'layer_5/output', 'layer_6/expansion_output', 'layer_6/depthwise_output', 'layer_6/output', 'layer_7/expansion_output', 'layer_7/depthwise_output', 'layer_7/output', 'layer_8/expansion_output', 'layer_8/depthwise_output', 'layer_8/output', 'layer_9/expansion_output', 'layer_9/depthwise_output', 'layer_9/output', 'layer_10/expansion_output', 'layer_10/depthwise_output', 'layer_10/output', 'layer_11/expansion_output', 'layer_11/depthwise_output', 'layer_11/output', 'layer_12/expansion_output', 'layer_12/depthwise_output', 'layer_12/output', 'layer_13/expansion_output', 'layer_13/depthwise_output', 'layer_13/output', 'layer_14/expansion_output', 'layer_14/depthwise_output', 'layer_14/output', 'layer_15/expansion_output', 'layer_15/depthwise_output', 'layer_15/output', 'layer_16/expansion_output', 'layer_16/depthwise_output', 'layer_16/output', 'layer_17/expansion_output', 'layer_17/depthwise_output', 'layer_17/output', 'layer_18/expansion_output', 'layer_18/depthwise_output', 'layer_18/output', 'global_pool', 'Logits', 'Predictions'])\n",
            "dict_keys(['layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_2/depthwise_output', 'layer_2/output', 'layer_3/expansion_output', 'layer_3/depthwise_output', 'layer_3/output', 'layer_4/expansion_output', 'layer_4/depthwise_output', 'layer_4/output', 'layer_5/expansion_output', 'layer_5/depthwise_output', 'layer_5/output', 'layer_6/expansion_output', 'layer_6/depthwise_output', 'layer_6/output', 'layer_7/expansion_output', 'layer_7/depthwise_output', 'layer_7/output', 'layer_8/expansion_output', 'layer_8/depthwise_output', 'layer_8/output', 'layer_9/expansion_output', 'layer_9/depthwise_output', 'layer_9/output', 'layer_10/expansion_output', 'layer_10/depthwise_output', 'layer_10/output', 'layer_11/expansion_output', 'layer_11/depthwise_output', 'layer_11/output', 'layer_12/expansion_output', 'layer_12/depthwise_output', 'layer_12/output', 'layer_13/expansion_output', 'layer_13/depthwise_output', 'layer_13/output', 'layer_14/expansion_output', 'layer_14/depthwise_output', 'layer_14/output', 'layer_15/expansion_output', 'layer_15/depthwise_output', 'layer_15/output', 'layer_16/expansion_output', 'layer_16/depthwise_output', 'layer_16/output', 'layer_17/expansion_output', 'layer_17/depthwise_output', 'layer_17/output', 'layer_18/expansion_output', 'layer_18/depthwise_output', 'layer_18/output', 'global_pool', 'Logits', 'Predictions'])\n",
            "dict_keys(['layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_2/depthwise_output', 'layer_2/output', 'layer_3/expansion_output', 'layer_3/depthwise_output', 'layer_3/output', 'layer_4/expansion_output', 'layer_4/depthwise_output', 'layer_4/output', 'layer_5/expansion_output', 'layer_5/depthwise_output', 'layer_5/output', 'layer_6/expansion_output', 'layer_6/depthwise_output', 'layer_6/output', 'layer_7/expansion_output', 'layer_7/depthwise_output', 'layer_7/output', 'layer_8/expansion_output', 'layer_8/depthwise_output', 'layer_8/output', 'layer_9/expansion_output', 'layer_9/depthwise_output', 'layer_9/output', 'layer_10/expansion_output', 'layer_10/depthwise_output', 'layer_10/output', 'layer_11/expansion_output', 'layer_11/depthwise_output', 'layer_11/output', 'layer_12/expansion_output', 'layer_12/depthwise_output', 'layer_12/output', 'layer_13/expansion_output', 'layer_13/depthwise_output', 'layer_13/output', 'layer_14/expansion_output', 'layer_14/depthwise_output', 'layer_14/output', 'layer_15/expansion_output', 'layer_15/depthwise_output', 'layer_15/output', 'layer_16/expansion_output', 'layer_16/depthwise_output', 'layer_16/output', 'layer_17/expansion_output', 'layer_17/depthwise_output', 'layer_17/output', 'layer_18/expansion_output', 'layer_18/depthwise_output', 'layer_18/output', 'global_pool', 'Logits', 'Predictions'])\n",
            "DONE WITH EPOCH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdONR2WTbCFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}